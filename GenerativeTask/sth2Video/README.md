# sth2Video 

# Data
- Public dataset:
   

- Synthesis data:
   
- API labeling:
  

# Models

- genVideo:
    - SORA(Sota), Make-A-Video, SVD-stabilityai, Gen-2, Pikalabs, VideoGen, Tune-A-Video, stable-diffusion-videos, Text2Video-Zero, CogVideo, https://arxiv.org/pdf/2310.10647.pdf, ChenHsing/Awesome-Video-Diffusion-Models

- gen image:
    - SD+ControlNet+EbSynth+Fusion, ControlNet, dreamshaper, https://civitai.com/models, stable-diffusion-webui, DiT, midjourney, mov2mov, stabilityai/stable-diffusion-xl-base-1.0, Diffusers, Dreambooth/ImageGen\
    - Diffusion: MDTv2(DiT), DiffiT, DiT, ADM, BIGRoC, simple diffusion(Unet/UVit), LDM, 	CDM, Improved DDPM
    - VAE: MAGVIT-v2, RQ-Transformer, RCG-L, MaskGIT
    - Autoregressive: VAR-VisualAutoregressive, RQ-Transformer, VQGAN+Transformer
    - GAN: StyleSAN, StyleGAN, Poly-INRm, GigaGAN, BigGAN-deep
    - Flow Matching: LFM

- gen 3d:
    - PointÂ·E, Wonder3D, HoloDiffusion
